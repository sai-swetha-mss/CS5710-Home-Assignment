# -*- coding: utf-8 -*-
"""CS5710 Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ktMAxuS_iLy12Poc1d2XPsocfkzNl1hP
"""

import numpy as np
import matplotlib.pyplot as plt

# 1. Generate dataset
np.random.seed(42)
X = np.random.uniform(0, 5, 200)
epsilon = np.random.normal(0, 1, 200)
y = 3 + 4 * X + epsilon

# Reshape X to (n,1) for matrix operations
X_b = np.c_[np.ones((200, 1)), X]  # add bias term

# 2. Closed-form solution
theta_closed = np.linalg.inv(X_b.T @ X_b) @ (X_b.T @ y)
intercept_closed, slope_closed = theta_closed
print("Closed-form solution:")
print(f"Intercept: {intercept_closed:.4f}, Slope: {slope_closed:.4f}")

# 3. Gradient Descent implementation
theta = np.zeros(2)  # [intercept, slope]
eta = 0.05
n_iter = 1000
m = len(y)

losses = []

for i in range(n_iter):
    y_pred = X_b @ theta
    residuals = y_pred - y
    gradients = (2/m) * (X_b.T @ residuals)
    theta -= eta * gradients
    mse = np.mean(residuals**2)
    losses.append(mse)

intercept_gd, slope_gd = theta
print("\nGradient Descent solution:")
print(f"Intercept: {intercept_gd:.4f}, Slope: {slope_gd:.4f}")

# 4. Plots
plt.figure(figsize=(14,5))

# (a) Raw data + fitted lines
plt.subplot(1,2,1)
plt.scatter(X, y, alpha=0.6, label="Raw data")
x_range = np.linspace(0, 5, 100)
plt.plot(x_range, intercept_closed + slope_closed * x_range, "r-", lw=2, label="Closed-form")
plt.plot(x_range, intercept_gd + slope_gd * x_range, "g--", lw=2, label="Gradient Descent")
plt.xlabel("x"); plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

# (b) Loss curve
plt.subplot(1,2,2)
plt.plot(losses)
plt.xlabel("Iteration")
plt.ylabel("MSE Loss")
plt.title("Gradient Descent Loss Curve")

plt.show()

# 5. Comment
print("\nComparison:")
print("Closed-form and Gradient Descent converge to nearly identical intercept and slope.")